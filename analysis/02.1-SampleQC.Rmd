---
title: "Quality control with SampleQC"
author: "Katharina Hembach"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, autodep = TRUE, cache = TRUE, dev = "png",
                      dev.args = list(png = list(type = "cairo")), 
                      message = FALSE)
```

### Load packages
```{r, message = FALSE}
library(SampleQC)
library(dplyr)
library(patchwork)
library(SingleCellExperiment)
```

Here, I am testing the `SampleQC` [package](https://github.com/wmacnair/SampleQC) from Will Macnair.

## Load data and preparation

```{r}
sce <- readRDS(file.path("output", "sce_02_quality_control.rds"))
qc_df <- colData(sce) %>% as.data.frame() %>%
  dplyr::mutate(cell_id = paste(barcode, sample_id, sep = "."),
                log_counts = log10(total), log_feats = log10(detected), 
                logit_mito = qlogis( (subsets_Mt_sum + 1)/(total + 2) ) )

qc_dt <- make_qc_dt(qc_df)
```

## Running SampleQC

```{r}
# which QC metrics do we want to use? (the most important bit)
qc_names <- c('log_counts', 'log_feats', 'logit_mito')
# which discrete-valued variables do we want to annotate the samples with?
annot_discrete <- c("group_id")

## Calculate distances between all samples and run dimension reduction
mmd_list <- calculate_sample_to_sample_MMDs(qc_dt, qc_names, 
                                            subsample=200, n_times=20, 
                                            n_cores=20)
mmd_list <- embed_sample_to_sample_MMDs(mmd_list, qc_dt, annot_discrete, 
                                        n_nhbrs=5)
print(table(mmd_list$mmd_clusts))

## We fit Gaussian mixture models to the whole dataset, because we only have 
## 6 samples
em_list <- fit_sampleQC(mmd_list, qc_dt, qc_names, K_all=1)
```

## Report setup

Following report is copied from Will's github [repository](https://github.com/wmacnair/SampleQC).

```{r setup_vars}
# define what to use and annotate
qc_names        = em_list[[1]]$qc_names
cluster_names   = names(em_list)
n_clusters      = length(em_list)
```

# Checking for outliers and QC batches via MMD

_Maximum mean discrepancy_ (MMD) is a measure of dissimilarity of empirical (possibly multivariate) distributions. If $X$ and $Y$ are sampled from distributions $D_x$ and $D_y$, then $E(MMD(X,Y)) = 0$ if and only if $D_x = D_y$. `SampleQC` uses MMD to estimate similarities between the QC matrices of samples in a experiment. Viewed as equivalent to a distance, `SampleQC` uses the MMD values as input to multiple non-linear embedding approaches, and for clustering. This then allows users to identify possible batch effects in the samples, and groupings of samples which have similar distributions of QC metrics.

## Plot MMD dissimilarity matrix

Heatmap of all pairwise dissimilarities between samples (values close to 0 indicate similar samples; values of 1 and higher indicate extremely dissimilar samples).

```{r plot_mmd_distns_heatmap, fig.height=6, fig.width=7}
(plot_mmd_heatmap(mmd_list))
```

## Plot over UMAP embedding with annotations{.tabset}

UMAP embedding of dissimilarity matrix, annotated with selected discrete and continuous values for each sample.

```{r plot_over_umap, fig.height=4, fig.width=5, results='asis'}
plot_embeddings(mmd_list, "discrete", "UMAP")
plot_embeddings(mmd_list, "continuous", "UMAP")
```

## Plot over MDS embedding with annotations{.tabset}

Multidimensional scaling (MDS) embedding of dissimilarity matrix, annotated with selected discrete and continuous values for each sample.

```{r plot_over_mds, fig.height=4, fig.width=5, results='asis'}
plot_embeddings(mmd_list, "discrete", "MDS")
plot_embeddings(mmd_list, "continuous", "MDS")
```

# Plot `SampleQC` model fits and outliers over QC biaxials

These plots show biaxial distributions of each sample, annotated with both the fitted mean and covariance matrices, and the cells which are then identified as outliers. You can use this to check that you have the correct number of components for each sample grouping, and to check that the fitting procedure has worked properly. The means and covariances of the components should match up to the densest parts of the biaxial plots.

```{r plot_histograms_split, fig.height=6, fig.width=7, results='asis'}
alpha_cut   = 0.001
for (ii in 1:n_clusters) {
    cat('## ', cluster_names[ii], '{.tabset}\n')
    for (s in sort(em_list[[ii]]$sample_list)) {
        cat('### ', s, ' \n')
        g_fit   = plot_fit_over_biaxials_one_sample(em_list[[ii]], qc_dt, s, qc_names, alpha_cut)
        g_out   = plot_outliers_one_sample(em_list[[ii]], s)
        g       = g_fit / g_out
        print(g)
        cat('\n\n')
    }
}
```

# Plot parameters

These plots show the fitted parameters for each sample and each mixture component. There are two sets of parameters: $\alpha_j$, the mean shift for each sample; and $(\mu_k, \Sigma_k)$, the relative means and covariances for each mixture component. 

## $\alpha_j$ values{.tabset}

Values of $\alpha_j$ which are extreme relative to those for most other samples indicate samples which are either outliers in terms of their QC statistics, or have been allocated to the wrong sample grouping.

```{r outputs_alpha_j_likes, fig.height=9, fig.width=8, results='asis'}
# plot likelihoods
for (i in 1:n_clusters) {
    cat('### ', cluster_names[i], '\n')
    print(plot_alpha_js_likelihoods(em_list[[i]]))
    cat('\n\n')
}
```

These plots show the same $\alpha_j$ values, but as biaxials, and equivalently for PCA projections.

## $\alpha_j$ PCA values{.tabset}

```{r outputs_alpha_j_pca, fig.height=4, fig.width=10, results='asis'}
for (i in 1:n_clusters) {
    cat('### ', cluster_names[i], ' feats\n')
    print(plot_alpha_js(em_list[[i]], qc_idx=1:2, pc_idx=1:2))
    cat('\n\n')
    cat('### ', cluster_names[i], ' mito\n')
    print(plot_alpha_js(em_list[[i]], qc_idx=c(1,3), pc_idx=c(1,3)))
    cat('\n\n')
}
```

These plots show the composition of each sample in terms of the $K$ different mixture components, plus outliers.

## $\beta_k$ values{.tabset}

```{r outputs_beta_k, fig.height=10, fig.width=6, results='asis'}
# plot likelihoods
for (i in 1:n_clusters) {
    cat('### ', cluster_names[i], '\n')
    print(plot_beta_ks(em_list[[i]]))
    cat('\n\n')
}
```

